{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CH. 7 - TOPIC MODELS\n",
    "## Activities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 7.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not necessary\n",
    "# added to suppress warnings coming from pyLDAvis\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/past/builtins/misc.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Mapping\n"
     ]
    }
   ],
   "source": [
    "import langdetect  # language detection\n",
    "import matplotlib.pyplot  # plotting\n",
    "import nltk  # natural language processing\n",
    "import numpy  # arrays and matrices\n",
    "import pandas  # dataframes\n",
    "import pyLDAvis  # plotting\n",
    "import pyLDAvis.sklearn  # plotting\n",
    "import regex  # regular expressions\n",
    "import sklearn  # machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path\n",
    "\n",
    "path = '~/Documents/packt-data/topic-model-health-tweets/latimeshealth.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "df = pandas.read_csv(path, sep=\"|\", header=None)\n",
    "df.columns = [\"id\", \"datetime\", \"tweettext\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define quick look function for data frame\n",
    "\n",
    "def dataframe_quick_look(df, nrows):\n",
    "    print(\"SHAPE:\\n{shape}\\n\".format(shape=df.shape))\n",
    "    print(\"COLUMN NAMES:\\n{names}\\n\".format(names=df.columns))\n",
    "    print(\"HEAD:\\n{head}\\n\".format(head=df.head(nrows)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE:\n",
      "(4171, 3)\n",
      "\n",
      "COLUMN NAMES:\n",
      "Index(['id', 'datetime', 'tweettext'], dtype='object')\n",
      "\n",
      "HEAD:\n",
      "                   id                        datetime  \\\n",
      "0  576760256031682561  Sat Mar 14 15:02:15 +0000 2015   \n",
      "1  576715414811471872  Sat Mar 14 12:04:04 +0000 2015   \n",
      "\n",
      "                                           tweettext  \n",
      "0  Five new running shoes that aim to go the extr...  \n",
      "1  Gym Rat: Disq class at Crunch is intense worko...  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe_quick_look(df, nrows=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEADLINES:\n",
      "['Five new running shoes that aim to go the extra mile http://lat.ms/1ELp3wU', 'Gym Rat: Disq class at Crunch is intense workout on pulley system http://lat.ms/1EKOFdr', 'Noshing through thousands of ideas at Natural Products Expo West http://lat.ms/1EHqywg', 'Natural Products Expo also explores beauty, supplements and more http://lat.ms/1EHqyfE', 'Free Fitness Weekends in South Bay beach cities aim to spark activity http://lat.ms/1EH3SMC']\n",
      "\n",
      "LENGTH:\n",
      "4171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view final data that will be carried forward\n",
    "\n",
    "raw = df['tweettext'].tolist()\n",
    "print(\"HEADLINES:\\n{lines}\\n\".format(lines=raw[:5]))\n",
    "print(\"LENGTH:\\n{length}\\n\".format(length=len(raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for checking language of tweets\n",
    "# filter to english only\n",
    "\n",
    "def do_language_identifying(txt):\n",
    "    try:\n",
    "       the_language = langdetect.detect(txt)\n",
    "    except:\n",
    "       the_language = 'none'\n",
    "    return the_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to perform lemmatization\n",
    "\n",
    "def do_lemmatizing(wrd):\n",
    "    out = nltk.corpus.wordnet.morphy(wrd)\n",
    "    return (wrd if out is None else out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to cleaning tweet data\n",
    "\n",
    "def do_tweet_cleaning(txt):\n",
    "    # identify language of tweet\n",
    "    # return null if language not english\n",
    "    lg = do_language_identifying(txt)\n",
    "    if lg != 'en':\n",
    "        return None\n",
    "    \n",
    "    # split the string on whitespace\n",
    "    out = txt.split(' ')\n",
    "    \n",
    "    # identify screen names\n",
    "    # replace with SCREENNAME\n",
    "    out = ['SCREENNAME' if i.startswith('@') else i for i in out]\n",
    "    \n",
    "    # identify urls\n",
    "    # replace with URL\n",
    "    out = [\n",
    "        'URL' if bool(regex.search('http[s]?://', i)) \n",
    "        else i for i in out\n",
    "    ]\n",
    "    \n",
    "    # remove all punctuation\n",
    "    out = [regex.sub('[^\\\\w\\\\s]|\\n', '', i) for i in out]\n",
    "    \n",
    "    # make all non-keywords lowercase\n",
    "    keys = ['SCREENNAME', 'URL']\n",
    "    out = [i.lower() if i not in keys else i for i in out]\n",
    "    \n",
    "    # remove keywords\n",
    "    out = [i for i in out if i not in keys]\n",
    "    \n",
    "    # remove stopwords\n",
    "    list_stop_words = nltk.corpus.stopwords.words('english')\n",
    "    list_stop_words = [regex.sub('[^\\\\w\\\\s]', '', i) for i in list_stop_words]\n",
    "    \n",
    "    out = [i for i in out if i not in list_stop_words]\n",
    "    \n",
    "    # lemmatizing\n",
    "    out = [do_lemmatizing(i) for i in out]\n",
    "    \n",
    "    # keep words 4 or more characters long\n",
    "    out = [i for i in out if len(i) >= 5]\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply cleaning function to every tweet\n",
    "\n",
    "clean = list(map(do_tweet_cleaning, raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEADLINES:\n",
      "[['running', 'shoes', 'extra'], ['class', 'crunch', 'intense', 'workout', 'pulley', 'system'], ['thousand', 'natural', 'product'], ['natural', 'product', 'explore', 'beauty', 'supplement'], ['fitness', 'weekend', 'south', 'beach', 'spark', 'activity']]\n",
      "\n",
      "LENGTH:\n",
      "4093\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# remove none types\n",
    "\n",
    "clean = list(filter(None.__ne__, clean))\n",
    "print(\"HEADLINES:\\n{lines}\\n\".format(lines=clean[:5]))\n",
    "print(\"LENGTH:\\n{length}\\n\".format(length=len(clean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn tokens back into strings\n",
    "# concatenate using whitespaces\n",
    "\n",
    "clean_sentences = [\" \".join(i) for i in clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running shoes extra', 'class crunch intense workout pulley system', 'thousand natural product', 'natural product explore beauty supplement', 'fitness weekend south beach spark activity', 'kayla harrison sacrifice', 'sonic treatment alzheimers disease', 'ultrasound brain restore memory alzheimers needle onlyso farin mouse', 'apple researchkit really medical research', 'warning chantix drink taking might remember']\n"
     ]
    }
   ],
   "source": [
    "print(clean_sentences[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activity 7.01 Unit Test\n",
    "\n",
    "def unittest_activity_7_01(predicted):\n",
    "    # testing presence of df\n",
    "    try:\n",
    "        df\n",
    "    except NameError:\n",
    "        print(\"No dataframe present.\")\n",
    "    \n",
    "    # testing expected length of clean sentences\n",
    "    actualCnt = 4093\n",
    "    predictedCnt = len(predicted)\n",
    "    assert actualCnt == predictedCnt, \"List lengths not equal.\"\n",
    "\n",
    "unittest_activity_7_01(predicted=clean_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 7.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define global variables\n",
    "\n",
    "number_words = 10\n",
    "number_docs = 10\n",
    "number_features = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 320)\t1\n"
     ]
    }
   ],
   "source": [
    "# bag of words conversion\n",
    "# count vectorizer (raw counts)\n",
    "\n",
    "vectorizer1 = sklearn.feature_extraction.text.CountVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    max_df=0.95, \n",
    "    min_df=10, \n",
    "    max_features=number_features\n",
    ")\n",
    "clean_vec1 = vectorizer1.fit_transform(clean_sentences)\n",
    "print(clean_vec1[0])\n",
    "\n",
    "feature_names_vec1 = vectorizer1.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to calculate perplexity based on number of topics\n",
    "\n",
    "def perplexity_by_ntopic(data, ntopics):\n",
    "    output_dict = {\n",
    "        \"Number Of Topics\": [], \n",
    "        \"Perplexity Score\": []\n",
    "    }\n",
    "    \n",
    "    for t in ntopics:\n",
    "        lda = sklearn.decomposition.LatentDirichletAllocation(\n",
    "            n_components=t,\n",
    "            learning_method=\"online\",\n",
    "            random_state=0\n",
    "        )\n",
    "        lda.fit(data)\n",
    "        \n",
    "        output_dict[\"Number Of Topics\"].append(t)\n",
    "        output_dict[\"Perplexity Score\"].append(lda.perplexity(data))\n",
    "        \n",
    "    output_df = pandas.DataFrame(output_dict)\n",
    "    \n",
    "    index_min_perplexity = output_df[\"Perplexity Score\"].idxmin()\n",
    "    output_num_topics = output_df.loc[\n",
    "        index_min_perplexity,  # index\n",
    "        \"Number Of Topics\"  # column\n",
    "    ]\n",
    "        \n",
    "    return (output_df, output_num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute function on vector of numbers of topics\n",
    "# takes several minutes\n",
    "\n",
    "df_perplexity, optimal_num_topics = perplexity_by_ntopic(\n",
    "    clean_vec1, \n",
    "    ntopics=[i for i in range(1, 21) if i % 2 == 0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Number Of Topics  Perplexity Score\n",
      "0                 2        350.450274\n",
      "1                 4        400.851077\n",
      "2                 6        426.428279\n",
      "3                 8        462.129327\n",
      "4                10        473.725037\n",
      "5                12        480.092033\n",
      "6                14        493.971335\n",
      "7                16        503.821238\n",
      "8                18        518.832303\n",
      "9                20        523.589597\n"
     ]
    }
   ],
   "source": [
    "print(df_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='online', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=2, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=0, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define and fit lda model\n",
    "\n",
    "lda = sklearn.decomposition.LatentDirichletAllocation(\n",
    "    n_components=optimal_num_topics,\n",
    "    learning_method=\"online\",\n",
    "    random_state=0\n",
    ")\n",
    "lda.fit(clean_vec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to format raw output into nice tables\n",
    "\n",
    "def get_topics(mod, vec, names, docs, ndocs, nwords):\n",
    "    # word to topic matrix\n",
    "    W = mod.components_\n",
    "    W_norm = W / W.sum(axis=1)[:, numpy.newaxis]\n",
    "    # topic to document matrix\n",
    "    H = mod.transform(vec)\n",
    "    \n",
    "    W_dict = {}\n",
    "    H_dict = {}\n",
    "    \n",
    "    for tpc_idx, tpc_val in enumerate(W_norm):\n",
    "        topic = \"Topic{}\".format(tpc_idx)\n",
    "        \n",
    "        # formatting w\n",
    "        W_indices = tpc_val.argsort()[::-1][:nwords]\n",
    "        W_names_values = [\n",
    "            (round(tpc_val[j], 4), names[j]) \n",
    "            for j in W_indices\n",
    "        ]\n",
    "        W_dict[topic] = W_names_values\n",
    "        \n",
    "        # formatting h\n",
    "        H_indices = H[:, tpc_idx].argsort()[::-1][:ndocs]\n",
    "        H_names_values = [\n",
    "            (round(H[:, tpc_idx][j], 4), docs[j]) \n",
    "            for j in H_indices\n",
    "        ]\n",
    "        H_dict[topic] = H_names_values\n",
    "        \n",
    "    W_df = pandas.DataFrame(\n",
    "        W_dict, \n",
    "        index=[\"Word\" + str(i) for i in range(nwords)]\n",
    "    )\n",
    "    H_df = pandas.DataFrame(\n",
    "        H_dict,\n",
    "        index=[\"Doc\" + str(i) for i in range(ndocs)]\n",
    "    )\n",
    "        \n",
    "    return (W_df, H_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get nice tables\n",
    "\n",
    "W_df, H_df = get_topics(\n",
    "    mod=lda,\n",
    "    vec=clean_vec1,\n",
    "    names=feature_names_vec1,\n",
    "    docs=raw,\n",
    "    ndocs=number_docs, \n",
    "    nwords=number_words\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Topic0                Topic1\n",
      "Word0      (0.0405, latfit)         (0.05, study)\n",
      "Word1        (0.034, study)      (0.0317, cancer)\n",
      "Word2      (0.0325, health)     (0.0226, patient)\n",
      "Word3      (0.0233, people)       (0.0179, death)\n",
      "Word4       (0.0196, could)       (0.0172, heart)\n",
      "Word5       (0.0186, brain)     (0.0154, disease)\n",
      "Word6  (0.0175, researcher)   (0.015, healthcare)\n",
      "Word7       (0.0173, woman)      (0.0148, weight)\n",
      "Word8   (0.0143, scientist)  (0.0147, california)\n",
      "Word9    (0.0133, american)     (0.0128, medical)\n"
     ]
    }
   ],
   "source": [
    "# word-topic table\n",
    "\n",
    "print(W_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Topic0  \\\n",
      "Doc0  (0.9442, Want your legs to look good in those ...   \n",
      "Doc1  (0.9373, RT @aminawrite: This little boy was b...   \n",
      "Doc2  (0.9373, Are humans wired to lie? In some situ...   \n",
      "Doc3  (0.9336, We're all in the clean-plate club, re...   \n",
      "Doc4  (0.9334, Spend time with dad this Father’s Day...   \n",
      "Doc5  (0.9315, RT @latimesscience: Bigger testicles ...   \n",
      "Doc6  (0.9304, Alcohol in movies linked to binge dri...   \n",
      "Doc7  (0.9284, Who is most sleep-deprived in America...   \n",
      "Doc8  (0.9284, RT @lisagirion: CA Med Board, under f...   \n",
      "Doc9  (0.9284, Colorectal cancers are falling among ...   \n",
      "\n",
      "                                                 Topic1  \n",
      "Doc0  (0.9497, Study of oil spill effects on tuna in...  \n",
      "Doc1  (0.9443, Have you suspected that stress makes ...  \n",
      "Doc2  (0.9412, Doctors often delay vaccines for youn...  \n",
      "Doc3  (0.9402, Does your dog know how you're feeling...  \n",
      "Doc4  (0.9354, Global warming may revive all kinds o...  \n",
      "Doc5  (0.935, MT @LATErynbrown Hepatitis C patients ...  \n",
      "Doc6  (0.9336, Four pool moves that can be cool summ...  \n",
      "Doc7  (0.9316, The \"cancer\" effect: When DCIS is cal...  \n",
      "Doc8  (0.9285, Great-grandchildren of rats injected ...  \n",
      "Doc9  (0.9285, Supplements to boost \"low T\" increase...  \n"
     ]
    }
   ],
   "source": [
    "# document-topic table\n",
    "\n",
    "print(H_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el223711123323240484107890790\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el223711123323240484107890790_data = {\"mdsDat\": {\"x\": [0.23814219580337595, -0.23814219580337595], \"y\": [0.0, 0.0], \"topics\": [1, 2], \"cluster\": [1, 1], \"Freq\": [51.09980376329194, 48.90019623670806]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\"], \"Freq\": [243.0, 195.0, 182.0, 129.0, 141.0, 112.0, 103.0, 119.0, 98.0, 105.0, 242.75510016188744, 194.65261314299372, 111.6554092625102, 104.67253768437199, 103.48117998652144, 79.40652146293228, 85.89328432365897, 68.8287742327819, 54.38024599067108, 58.92026642498735, 139.44998375749867, 117.52342864878256, 203.7352509344071, 181.61874056613658, 129.36589107420298, 98.35333280807099, 102.60467846099554, 87.99697469906819, 84.13823894185226, 86.26068789387566, 57.312389084786375, 73.62401162772781, 61.94317406009241, 84.82679053548799, 67.85985953730165, 286.74902620444726], \"Term\": [\"latfit\", \"health\", \"cancer\", \"patient\", \"people\", \"brain\", \"death\", \"could\", \"heart\", \"researcher\", \"latfit\", \"health\", \"brain\", \"researcher\", \"woman\", \"american\", \"scientist\", \"expert\", \"today\", \"might\", \"people\", \"could\", \"study\", \"cancer\", \"patient\", \"heart\", \"death\", \"disease\", \"california\", \"healthcare\", \"breast\", \"medical\", \"treatment\", \"weight\", \"research\", \"study\"], \"Total\": [243.0, 195.0, 182.0, 129.0, 141.0, 112.0, 103.0, 119.0, 98.0, 105.0, 243.53163998673776, 195.2828029413827, 112.30227015802603, 105.30803637060116, 104.26015033401025, 80.10210929320667, 86.70105656615492, 69.49304273189128, 55.01321448773498, 59.61504402223571, 141.20924327352733, 119.82266705973369, 490.4842771388544, 182.19528327008888, 129.95084128487377, 98.96039497378943, 103.32385642668308, 88.63226268875513, 84.83243110124585, 87.06236446474806, 57.85936855194224, 74.3285547790555, 62.59161535994166, 86.13577191492283, 69.42500712699034, 490.4842771388544], \"loglift\": [10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.6682, 0.6682, 0.6656, 0.6653, 0.6639, 0.6627, 0.662, 0.6618, 0.6598, 0.6597, 0.6589, 0.652, -0.2072, 0.7122, 0.7109, 0.7092, 0.7084, 0.7082, 0.7072, 0.7061, 0.7059, 0.7059, 0.705, 0.7001, 0.6926, 0.1786], \"logprob\": [10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.2058, -3.4266, -3.9824, -4.047, -4.0584, -4.3233, -4.2447, -4.4662, -4.7018, -4.6216, -3.7601, -3.9312, -3.381, -3.4519, -3.7912, -4.0653, -4.023, -4.1765, -4.2214, -4.1965, -4.6053, -4.3549, -4.5276, -4.2132, -4.4364, -2.9952]}, \"token.table\": {\"Topic\": [1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2], \"Freq\": [0.9862411951079528, 0.012484065760860162, 0.9973084234396982, 0.008904539494997305, 0.017283285750038343, 0.9851472877521856, 0.011787944622340478, 0.9901873482766, 0.005488616291551224, 0.9989281650623227, 0.9847886288591369, 0.016691332692527743, 0.009678306971726164, 0.9968656180877948, 0.011282573294011946, 0.9928664498730512, 0.992905149745803, 0.014389929706460913, 0.9985518287472165, 0.005120778608960085, 0.01148601931670377, 0.9877976612365241, 0.010105052635095677, 0.9902951582393764, 0.9978169572267213, 0.004106242622332186, 0.013453779680938756, 0.9955796963894679, 0.9896830735878296, 0.01677428938284457, 0.0076952175923804475, 0.9926830694170777, 0.9843548253477433, 0.01416337878198192, 0.028808063301191376, 0.9794741522405067, 0.9970749015818972, 0.009495951443637116, 0.9919140943153328, 0.01153388481762015, 0.41591547274459995, 0.5851359837142166, 0.9815823434938369, 0.018177450805441423, 0.015976580796794634, 0.9905480094012673, 0.011609578433774415, 0.9868141668708252, 0.9879134038271267, 0.009591392270166279], \"Term\": [\"american\", \"american\", \"brain\", \"brain\", \"breast\", \"breast\", \"california\", \"california\", \"cancer\", \"cancer\", \"could\", \"could\", \"death\", \"death\", \"disease\", \"disease\", \"expert\", \"expert\", \"health\", \"health\", \"healthcare\", \"healthcare\", \"heart\", \"heart\", \"latfit\", \"latfit\", \"medical\", \"medical\", \"might\", \"might\", \"patient\", \"patient\", \"people\", \"people\", \"research\", \"research\", \"researcher\", \"researcher\", \"scientist\", \"scientist\", \"study\", \"study\", \"today\", \"today\", \"treatment\", \"treatment\", \"weight\", \"weight\", \"woman\", \"woman\"]}, \"R\": 10, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el223711123323240484107890790\", ldavis_el223711123323240484107890790_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el223711123323240484107890790\", ldavis_el223711123323240484107890790_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el223711123323240484107890790\", ldavis_el223711123323240484107890790_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iteractive plot\n",
    "# pca biplot and histogram\n",
    "\n",
    "lda_plot = pyLDAvis.sklearn.prepare(lda, clean_vec1, vectorizer1, R=10)\n",
    "pyLDAvis.display(lda_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activity 7.02 Unit Test\n",
    "\n",
    "def unittest_activity_7_02(num_topics):\n",
    "    # testing optimal number of topics\n",
    "    assert num_topics == 2, \"Number of optimal topics wrong.\"\n",
    "    \n",
    "    # testing presence of lda model\n",
    "    try:\n",
    "        lda\n",
    "    except:\n",
    "        print(\"No lda model defined.\")\n",
    "        \n",
    "    # testing presence of W_df\n",
    "    try:\n",
    "        W_df\n",
    "    except:\n",
    "        print(\"No W_df defined.\")\n",
    "        \n",
    "    # testing presence of H_df\n",
    "    try:\n",
    "        H_df\n",
    "    except:\n",
    "        print(\"No H_df defined.\")\n",
    "\n",
    "unittest_activity_7_02(num_topics=optimal_num_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity 7.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# bag of words conversion\n",
    "# tf-idf method\n",
    "\n",
    "vectorizer2 = sklearn.feature_extraction.text.TfidfVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    max_df=0.5, \n",
    "    min_df=20, \n",
    "    max_features=number_features,\n",
    "    smooth_idf=False\n",
    ")\n",
    "clean_vec2 = vectorizer2.fit_transform(clean_sentences)\n",
    "print(clean_vec2[0])\n",
    "\n",
    "feature_names_vec2 = vectorizer2.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.1, beta_loss='frobenius', init='nndsvda', l1_ratio=0.5,\n",
       "    max_iter=200, n_components=2, random_state=0, shuffle=False, solver='mu',\n",
       "    tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define and fit nmf model\n",
    "\n",
    "nmf = sklearn.decomposition.NMF(\n",
    "    n_components=optimal_num_topics,\n",
    "    init=\"nndsvda\",\n",
    "    solver=\"mu\",\n",
    "    beta_loss=\"frobenius\",\n",
    "    random_state=0, \n",
    "    alpha=0.1, \n",
    "    l1_ratio=0.5\n",
    ")\n",
    "nmf.fit(clean_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get nicely formatted result tables\n",
    "\n",
    "W_df, H_df = get_topics(\n",
    "    mod=nmf,\n",
    "    vec=clean_vec2,\n",
    "    names=feature_names_vec2,\n",
    "    docs=raw,\n",
    "    ndocs=number_docs, \n",
    "    nwords=number_words\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Topic0                Topic1\n",
      "Word0    (0.3724, study)      (0.5947, latfit)\n",
      "Word1   (0.0259, cancer)       (0.0483, steps)\n",
      "Word2   (0.0208, people)       (0.0444, today)\n",
      "Word3   (0.0185, health)      (0.04, exercise)\n",
      "Word4    (0.0184, brain)  (0.0272, healthtips)\n",
      "Word5  (0.0184, obesity)     (0.0257, workout)\n",
      "Word6  (0.0175, suggest)      (0.022, fitness)\n",
      "Word7   (0.0167, weight)     (0.0202, getting)\n",
      "Word8    (0.0159, woman)       (0.0142, great)\n",
      "Word9     (0.014, could)     (0.0131, morning)\n"
     ]
    }
   ],
   "source": [
    "# word-topic table\n",
    "\n",
    "print(W_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Topic0  \\\n",
      "Doc0  (0.2028, Knot Yet: Getting married later can h...   \n",
      "Doc1  (0.2028, RT @latimesscience: Estrogen helps fe...   \n",
      "Doc2  (0.2028, Resveratrol's anti-aging potential ge...   \n",
      "Doc3  (0.2028, Self-injury: Even little boys and gir...   \n",
      "Doc4  (0.2028, Study: Annual PSA screening doesn't r...   \n",
      "Doc5  (0.2028, Survey by @UCSF researchers finds no ...   \n",
      "Doc6  (0.2028, Cardio doesn't have to be dull — try ...   \n",
      "Doc7  (0.2028, Ruling out race in college admissions...   \n",
      "Doc8  (0.2028, USADA chief previews Lance Armstrong ...   \n",
      "Doc9  (0.2028, The sanitary benefits of fist-bumping...   \n",
      "\n",
      "                                                 Topic1  \n",
      "Doc0  (0.2272, RT @annagorman: One more from @AmerMe...  \n",
      "Doc1  (0.2272, Doctors find a simple way to manage h...  \n",
      "Doc2  (0.2272, RT @LATerynbrown: ER survey: Nearly 1...  \n",
      "Doc3  (0.2272, Volunteers wanted for cancer research...  \n",
      "Doc4  (0.2272, RT @annagorman: Hospital's ban on abo...  \n",
      "Doc5  (0.2272, Work your legs, butt and core while i...  \n",
      "Doc6  (0.2272, How the month you were conceived can ...  \n",
      "Doc7  (0.2272, Scientists find a way to tell if supp...  \n",
      "Doc8  (0.2272, So Elton John has \"appendix abscess s...  \n",
      "Doc9  (0.2272, RT @latimes: Live now: Video chat wit...  \n"
     ]
    }
   ],
   "source": [
    "# document-topic table\n",
    "\n",
    "print(H_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activity 7.03 Unit Test\n",
    "\n",
    "def unittest_activity_7_03():\n",
    "    # testing presence of lda model\n",
    "    try:\n",
    "        nmf\n",
    "    except:\n",
    "        print(\"No lda model defined.\")\n",
    "        \n",
    "    # testing presence of W_df\n",
    "    try:\n",
    "        W_df\n",
    "    except:\n",
    "        print(\"No W_df defined.\")\n",
    "        \n",
    "    # testing presence of H_df\n",
    "    try:\n",
    "        H_df\n",
    "    except:\n",
    "        print(\"No H_df defined.\")\n",
    "\n",
    "unittest_activity_7_03()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
